{"cells":[{"cell_type":"markdown","metadata":{"id":"pdN0PDukrVHt"},"source":["# Statistical learning concepts\n","\n","This notebook covers concepts from Chapter 2 in the [Introduction to Statistical Learning textbook](https://catalog.library.tamu.edu/Record/in00004735720).\n","\n","There is a myriad of AI and machine learning applications\n"," - Protein folding\n"," - Image generation\n"," - Climate forecasting\n","\n","These areas are incredibly different, but the way we craft models for each is largely the same.\n","\n","There are unifying principles for constructing and evaluating __learning systems__ to solve a vast array of problems.\n"," - Understanding these principles allows you to better understand when, how and why your model works\n"," - Develop new models and generalize old models, see commonalities across disciplines\n"," - Guide you towards more informed model choices\n","\n","\n","These principles are called __statistical learning theory__. SLT gives us a rigorous framework for\n","1. Defining __learning__ problems and building models\n","3. Analyzing the performance of machine learning models (theoretically and empirically)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1670,"status":"ok","timestamp":1724857177225,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"bJNWOLmRaljn","outputId":"a85cf90d-9ab4-414f-d7a9-e1c287c2cd8e"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# dont run this without changing the directory\n","\n","%cd '/content/drive/MyDrive/STAT335_Fall2024/statlearning/'"]},{"cell_type":"markdown","metadata":{"id":"18e7O0dUEfsg"},"source":["## 1. Statistical learning theory\n","\n","> SLT is the subfield of Statistics / Computer Science tries to understand when, how, and why machine learning algorithms work.\n","\n","Statistical learning is not the same as machine learning. Machine learning is about building models. Statistical learning is about understanding models.\n","- Understand the __predictive skill__ of __learning algorithms__\n","\n","To be an effective data scientist you need to know the basics of learning theory."]},{"cell_type":"markdown","metadata":{"id":"vxB5TH_6k3rB"},"source":["### Overview\n","\n","To get started in ML we need the following basics from SLT\n","\n","1. How to define a learning problem\n","2. How to solve learning problems\n","3. How to evaluate effectiveness"]},{"cell_type":"markdown","metadata":{"id":"A8yDwIbUAgr5"},"source":["## 1.1 How to define a learning problem\n","\n","> Machine learning is the art and science of building __models__ that __learn__ from data to accomplish a set of tasks\n","\n","Lets reformulate this nebulous goal into something a little more concrete.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mI43wFs-m3SS"},"source":["### Tasks\n","\n","All learning problems start with a __task__. A task is something that you would like to do\n"," - Classify images\n"," - Predict bitcoin price\n","\n","but, typically, is hard or inconvenient to do.\n","\n","In this class we will focus almost exclusively on tasks that involve __predicting__ one random variable $Y$ from another random variable $X$."]},{"cell_type":"markdown","metadata":{"id":"_6pB_v1-m5lO"},"source":["### Solving tasks\n","\n","Suppose we have a task that we want to complete (or automate). We're going to teach a machine to do it for us!\n","\n","What does a machine need to learn how to solve a task?\n","\n","1. Data\n","2. Model family\n","3. Loss function"]},{"cell_type":"markdown","metadata":{"id":"Y1JomWBHfbU4"},"source":["## 1.2 Data\n","\n","Task: Our goal is to predict $Y$ given $X$.\n","\n","- Ex. For a medical diagnosis, $X$ might record the outcome of a medical test and $Y$ could be whether you have a disease or not. $Y = $ {0, 1}\n","\n","- Ex. In image classification, $X$ is a picture of an object and $Y$ is the name of the object. $Y = $ {house, boat, car, horse, etc...}\n","\n","- Ex. For housing price forecasting, $X$ could record the features of your house (size, number of bathrooms/bedrooms, previous price, etc.) and $Y$ is the price tomorrow. $Y = [0, \\infty)$"]},{"cell_type":"markdown","metadata":{"id":"8pbSP1hb9Jnj"},"source":["### Collecting data\n","\n","\n","- Do not know the exact relationship between $X$ and $Y$.\n","  - We do not know the exact joint distribution of $(X, Y)$\n","  - or the conditional distribution $Y \\mid X$\n","\n","- Instead, suppose we can sample the distribution of $(X, Y)$\n"," - that is we have access to instances of $(x_i, y_i)$ drawn from the joint distribution of $(X, Y)$\n"," - $n$ independent draws $\\{(x_1,y_i),...,(x_n,y_n)\\}$ from the joint distribution of $(X, Y)$.\n"," - We call this __training data__"]},{"cell_type":"markdown","metadata":{"id":"lyEez-6KmBg9"},"source":["Because we want to learn the relationship between $X$ and $Y$ we need to gather two types of data.\n","\n","1. __Targets__ $y_1,...,y_n$.\n","  - Otherwise known as labels or dependent variables.\n","  - This is our ultimate object of interest. we want to be able to predict future values say $y_{n+1}$\n","\n","2. __Features__ $x_1,...,x_n$.\n","  - Otherwise known as covariates or independent variables.\n","  - This is the information that we use to predict $y$.\n","  - In statistical learning, the more covariate information the better. Not as interested in selecting (subsetting) variables as in classical statistics but rather quantifying feature importance.\n","  - We assume there is some relationship between $x_i$ and $y_i$"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"--Iu_SwNmWBM"},"source":["- We will use the notation $X$ to refer to a random quantity and $x$ to refer to a fixed quantity (data).\n","\n","- Abstractly we're interested in the relationship between $X$ and $Y$ but we have to settle for the relationship between $x_1,...,x_n$ and $y_1,...,y_n$.\n"," - And hope the learned relationship __generalizes__ to future data points\n"," - i.e. that it works for the entire process\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8LswE1qrVHt"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import math"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":742},"executionInfo":{"elapsed":815,"status":"ok","timestamp":1724857178038,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"-zCTS7pBBFVG","outputId":"463b51e5-6055-4579-82c7-c0fa9d74d9ed"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["from scipy.stats import multivariate_normal\n","\n","x, y = np.mgrid[-3:4:.01, -2:2:.01]\n","pos = np.dstack((x, y))\n","ideal = multivariate_normal([0.5, -0.2], [[2.0, 0.8], [0.8, 0.5]])\n","\n","fig, ax = plt.subplots(1, 2, constrained_layout = True, figsize = (12, 5))\n","ax[0].contour(x, y, ideal.pdf(pos))\n","ax[0].set_xlabel('X', fontsize = 20)\n","ax[0].set_ylabel('Y', fontsize = 20)\n","ax[0].set_xlim((-4, 5))\n","ax[0].set_ylim((-3, 3))\n","ax[0].set_title('Ideal: joint distribution', fontsize = 16)\n","\n","np.random.seed(0)\n","realistic = np.random.multivariate_normal([0.5, -0.2], [[2.0, 0.8], [0.8, 0.5]], 500)\n","ax[1].scatter(realistic[:,0], realistic[:,1])\n","ax[1].set_xlabel('X', fontsize = 20)\n","ax[1].set_ylabel('Y', fontsize = 20)\n","ax[1].set_xlim((-4, 5))\n","ax[1].set_ylim((-3, 3))\n","ax[1].set_title('Real: random draws (data)', fontsize = 16)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fAQYkVgUs7tX"},"source":["\n","The process of collecting _good_ data is very challenging.\n","\n","We will assume that we have access to data already and that it meets the following conditions\n","1. Each $x_i \\in \\mathbb{R}^p$ and $y_i \\in \\mathbb{R}^q$\n","1. Any pair of observations $(x_i, y_i)$ were collected independently of any $(x_j, y_j)$ for $i \\neq j$\n","2. $(x_i, y_i)$ and $(x_j, y_j)$ have the same distribution for all $i, j$.\n","3. We have a fixed training sample size $n$, i.e. $1 \\leq i \\leq n$"]},{"cell_type":"markdown","metadata":{"id":"1WZkHPOqmlt5"},"source":["Note that all of these assumptions are regularly violated in practice. They just make theory a bit easier.\n"," - Condition 1 means that all inputs and outputs (features and targets) are real valued vectors (or can be represented as such).\n"," - Conditions 2 and 3 are collectively know as $i.i.d$ meaning __I__ndependent and __I__dentically __D__istributed.\n"," - Condition 4 just means we do not plan to keep sampling and updating our model\n","\n","Unless stated otherwise we will assume our data meets these conditions\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uAWYseaJrGMl"},"source":["Here is another example of data that we will often work with. Pairs of images and labels.\n","- Each image $x_i$ is a 64 dimesional vector\n","- Each label $y_i$ is an integer from 0 to 9"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"elapsed":915,"status":"ok","timestamp":1724857178949,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"v-HGix-Oq4Gv","outputId":"e69e6342-f15f-478a-f78b-597abba9f36d"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["from sklearn import datasets\n","digits = datasets.load_digits()\n","x, y = digits.data, digits.target\n","\n","_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n","for ax, image, label in zip(axes, digits.images, digits.target):\n","    ax.imshow(image, cmap=plt.cm.gray_r)\n","    ax.set_title(\"Label: %i\" % label)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1724857178949,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"AiEjuc0m2MKr","outputId":"5326f1ae-38a7-4ae8-d3bd-11f4b0e1fe6b"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["image, label"]},{"cell_type":"markdown","metadata":{"id":"72Rq-VfihYXj"},"source":["## 1.3 Models and Learning\n","\n","We now have\n","1. A task (predict $Y$ given $X$)\n","2. Some training samples $\\{(x_1,y_i),...,(x_n,y_n)\\}$\n","\n","If we knew the joint distribution of $(X, Y)$\n","- we could find the conditional distribution $Y \\mid X$\n","- this would be the best we could do to describe the relationship between $X$ and $Y$\n","\n","Instead, we will have to settle for an approximation of the conditional distribution called a __model__ or __predictor__\n"]},{"cell_type":"markdown","metadata":{"id":"9eXdEIsIvjD8"},"source":["### A model\n","In theory a regression model can be any function $f$\n","\n","$$ f: X \\mapsto Y$$\n","\n","\n","Generally, our model will include an error term to represent our lack of certainty\n","\n","$$Y = f(X) + \\epsilon$$\n","\n","where $\\mathbb{E}[\\epsilon] = 0$ and $\\text{Var}(\\epsilon)= \\sigma^2$\n","\n","Given the the value $X$ we can \"predict\" $Y$ by simply feeding $X$ through the function $f$\n","\n","$$\\widehat{Y} = f(X)$$\n","\n","and ignoring the error term.  \n","\n","__For discussion__: Why is it OK to simply ignore the error term when formulating our prediction?"]},{"cell_type":"markdown","metadata":{"id":"GvGjAdQ0ie5z"},"source":["__Example:__ Linear regression\n","\n","In linear regression we assume a linear model (prediction function) from $X$ to $Y$ as\n","\n","$$ Y = \\alpha + \\beta X + \\epsilon$$\n","\n","and make predictions as\n","\n","$$\\widehat{Y} = \\alpha + \\beta X$$\n","\n","after we learn the values of $\\alpha$ and $\\beta$."]},{"cell_type":"markdown","metadata":{"id":"7Z3A4DoAnSD_"},"source":["### Model families\n","\n","As stated in the previous section we have to learn a statistical relationship, which means don't know the statistical relationship, which means we don't know what $f$ is\n","\n","Instead we can specify the __type of model__ we want to use. Formally, a pre-supposed __family of predictors__ $\\mathcal{F}$\n","  - These are all the possible predictors we will consider. Different problems require different families of predictors.\n","  - Ex. linear predictors $f \\in \\mathcal{F}$ s.t. $f(x) = \\alpha + \\beta x$\n","  - Impossible to learn an arbitrary predictor from features to targets\n","  - Instead we impose structure on the relationship between features and targets, such as a linear relationship.\n","  - $\\mathcal{F}$ is also known as a hypothesis class"]},{"cell_type":"markdown","metadata":{"id":"7DHmOUPtoJjs"},"source":["__Example:__ Linear regression\n","\n","We suppose $\\mathcal{F}$ is all linear models, i.e. models of the form $f(x) = \\alpha + \\beta x$ for $\\alpha, \\beta \\in \\mathbb{R}$. We model $Y$ as\n","\n","$$ Y = \\alpha + \\beta X + \\epsilon$$\n","\n","where $\\alpha$ and $\\beta$ are unknown. Some examples from this class are plotted below"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":768},"executionInfo":{"elapsed":759,"status":"ok","timestamp":1724858170571,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"7XpjJ_U1ovWN","outputId":"f0256668-413c-49c5-a64e-272da862842b"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["np.random.seed(1)\n","x = np.random.normal(1, 1, 200)\n","y = 0.5 * x + np.random.normal(0, 0.4, 200)\n","\n","alpha = np.round(np.random.uniform(-1, 1, 5), 2)\n","beta = np.round(np.random.uniform(-1, 1, 5).reshape(-1, 1), 2)\n","\n","lm = alpha[:,None] + beta @ x.reshape(1, -1)\n","lm = lm.T\n","\n","plt.figure(figsize = (9, 6))\n","plt.scatter(x, y, alpha = 0.25)\n","plt.plot(x, lm[:,0], label = f'a = {alpha[0]}, b = {beta[0][0]}')\n","plt.plot(x, lm[:,1], label = f'a = {alpha[1]}, b = {beta[1][0]}')\n","plt.plot(x, lm[:,2], label = f'a = {alpha[2]}, b = {beta[2][0]}')\n","plt.plot(x, lm[:,3], label = f'a = {alpha[3]}, b = {beta[3][0]}')\n","plt.plot(x, lm[:,4], label = f'a = {alpha[4]}, b = {beta[4][0]}')\n","plt.xlabel(\"X\", fontsize = 15)\n","plt.ylabel(\"Y\", fontsize = 15)\n","plt.title('Random Linear models', fontsize = 15)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"r7BFghqbqXFd"},"source":["Clearly some of the above lines \"fit\" the data better than others\n","\n","How do we determine which line is best? (including those not plotted)\n","\n","In other words, how do we find the optimal model $f$ from our class $\\mathcal{F}$?"]},{"cell_type":"markdown","metadata":{"id":"BY26OX4ycyw5"},"source":["### Choosing function classes (model type)\n","\n","- In general we do not know the appropriate function class, so you will often try many different types of models and see which fits best overall\n","  - Try linear, sinusoidal, exponential, etc.\n","\n","- Always assume your function class is __insufficient__\n","  - i.e., the \"true\" relationship is not in your supposed class\n","  - e.g., even when using the class of linear models, the relationship is  almost never truly linear\n","\n","- Being exactly correct is not the goal\n","  - Just want to find $f$ with a small loss...\n","  - ...that continues to perform well on future data\n","\n","\n","> \"All models are wrong, some are useful.\" -- George Box\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":558},"executionInfo":{"elapsed":527,"status":"ok","timestamp":1724857179933,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"oX17n3fjDdrS","outputId":"b1e430b3-1f94-4c69-fb7f-01944728a811"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":633},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1724857179933,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"KAGxMxA4DXgt","outputId":"dc2b1833-30ab-4001-cad9-aa5e5d5247ea"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1724857179934,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"nSXCu_DtD1Xh","outputId":"f5d54ed5-227f-455b-8a02-0bf749bc1502"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"vBixOh1JDJSq"},"source":["The type of model we choose can have an enormous impact on our ability to fit the data.\n","\n","Remember, in nearly all cases, the model class is __WRONG__.\n","\n","But clearly, some are more useful than others (i.e. can better approximate the true underlying relationship)."]},{"cell_type":"markdown","metadata":{"id":"--uCYx5CEfx_"},"source":["We will discuss ways of choosing models, but ultimately this will come down to experimentation, prior knowledge, and balancing the needs for flexibility, interpretability, and accuracy.\n","\n","The bulk of this class will focus on evaluating specific model classes and how to choose among different classes."]},{"cell_type":"markdown","metadata":{"id":"UBaVTk8eqqiN"},"source":["## 1.4 Loss functions\n","\n","We now have\n","1. A task (predict $Y$ given $X$)\n","2. Some training samples $\\{(x_1,y_i),...,(x_n,y_n)\\}$\n","3. A model family $\\mathcal{F} =\\{ f \\mid f:X \\mapsto Y \\}$\n","\n","The only outstanding issue is that we dont know which model $f \\in \\mathcal{F}$ to choose.\n","- We don't know which $f \\in \\mathcal{F}$ most accurately represents the relationship between $X$ and $Y$."]},{"cell_type":"markdown","metadata":{"id":"LDrgI5UlxwAa"},"source":["To find the optimal function $f$ we need some way to quantitatively measure how good $f$ is. We do this with a __loss function__.\n","\n","- A loss function is a mathematical function describing how well your model $f$ fits the data $(x_1, y_1),...,(x_n, y_n)$. We generically denote a loss function as\n","$$ \\mathcal{L}(f, (x, y)) $$\n","\n","- The loss function will return a (usually postive) real number. For example the mean square error loss from regression\n","\n","$$ \\mathcal{L}(f, (x, y)) = MSE(f, (x, y)) =  \\frac{1}{n} \\sum_{i = 1}^n (y_i - f(x_i))^2 $$\n","\n","- The closer $f(x_i)$ is to $y_i$ on average, the smaller $MSE(f, (x, y))$ will be."]},{"cell_type":"markdown","metadata":{"id":"7_FtTkH1wYN0"},"source":["__Example:__ Least Squares (MSE)\n","\n","https://www.geogebra.org/m/crBa6TAW"]},{"cell_type":"markdown","metadata":{"id":"LjVWVHtfwdG2"},"source":["### Optimal models\n","\n","The loss function is how we define and choose our optimal model. Essentially the optimal model is the model with the smallest loss. Mathematically\n","\n","$$ \\hat{f} = \\arg\\min_{f \\in \\mathcal{F}} \\mathcal{L}(f, (x, y))$$\n","\n","That is we choose $\\hat f \\in \\mathcal{F}$ such that\n","$$\n","\\mathcal{L}(\\hat f, (x, y)) \\leq \\mathcal{L}(f, (x, y)) \\quad \\forall f \\in \\mathcal{F}\n","$$\n","\n","__Example:__ Linear models with MSE\n","$$\\hat{\\alpha}, \\hat{\\beta} = \\arg\\min_{\\alpha, \\beta \\in \\mathbb{R}} \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\alpha - \\beta x_i)^2$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H0QYGlAd0hav"},"source":["###Choosing a loss function\n","\n","- The loss function is how we evaluate models in our model family\n","- Also defines what it means to be \"optimal\" (smallest loss)\n","- Different loss functions lead to different optimal models\n","   - because the definition of \"optimal\" changes\n","- Choosing a loss function is an integral part of model building\n","- Two example choices (regression)\n"," - $MSE(f, (x, y)) = \\frac{1}{n} \\sum_{i =1}^n (y_i - f(x_i))^2$\n"," - $MAE(f, (x, y)) = \\frac{1}{n} \\sum_{i =1}^n |y_i - f(x_i)|$\n","\n","- Properties of your loss function will determine properties of your selected model\n","  - $MAE$ is more robust to outliers than $MSE$, so the resulting model will not try to fit the outliers as closely as if it was trained by $MSE$\n","  - In fact, minimizing the $MSE$ fits the mean of the conditional distribution, $E[Y|X]$, while minimizing the $MAE$ fits the conditional median."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":558},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1724857180382,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"6NhFYWUm2tmX","outputId":"933428a9-6706-47e1-bc5d-f2653004cbae"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rp__Rw9Sq3HX"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["from scipy.optimize import minimize_scalar\n","\n","## optimize parameters (assume alpha = 0 because it is)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1724857180382,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"MgxtpwCc41tH","outputId":"ef1a7e86-7517-480f-d1d1-a5809feb01eb"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":553},"executionInfo":{"elapsed":439,"status":"ok","timestamp":1724857180815,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"2lxOi-Cd5NMB","outputId":"96b7d71f-714d-482a-c19b-497a62b6f956"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"Wmm4EBWzEtBe"},"source":["- Using the exact same model type (linear model with no intercept) changing the loss function has clearly changed the predictor.\n","\n","- In this case, the MAE loss estimated the conditional median while the MSE estimated the conditional mean.\n","\n","- Choice of loss function can have an even larger impact in more complex models.\n","\n","- The loss is literally our way of guiding the model fitting process by quantifying how to penalize differences between the estimated and observed values."]},{"cell_type":"markdown","metadata":{"id":"sikRyb9gqorm"},"source":["### Likelihood-based approaches\n","\n","In addition to choosing a loss function based on _which properties you want your algorithm to have_, we can choose a natural loss function via the __likelihood__.\n","\n","> A likelihood is a function specifying how likely particular parameter values are to be the ones that generated the data from this model class.\n","\n","#### Example\n","Suppose $X \\sim N(\\mu, \\sigma^2)$.  \n","- $X$ is the random variable.\n","- $\\theta = (\\mu, \\sigma)$ are the parameters.\n","- Likelihood $\\mathcal{L}(\\theta \\mid X=x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp((x - \\mu)^2 / (2 \\sigma^2))$\n","\n","The likelihood function and the density function have the same form.\n","$$\n","p(X \\mid \\theta) = \\mathcal{L}(\\theta \\mid X=x)\n","$$\n","However,\n","- The likelihood is a function of $\\theta$ with $X$ considered fixed ($X=x$).\n","- The density is a function of $X$ with $\\theta$ considered fixed."]},{"cell_type":"markdown","metadata":{"id":"Z-ApTzQWkEr5"},"source":["#### Maximizing likelihoods\n","\n","The likelihood is a function of $\\theta$, which means we can optimize it in terms of $\\theta$ (i.e. we can find the value $\\hat \\theta$ such that $\\mathcal{L}(\\hat \\theta \\mid x) \\geq \\mathcal{L}(\\theta \\mid x)$ for all $\\theta$). More on this later.\n","\n","\n","Finding $\\hat \\theta$ in this way is called __maximum likelihood estimation__ (MLE).\n","- Finds the parameter $\\hat \\theta$ which was \"most likely'' to have produced the data $x$\n","- _Asymptotic consistency_ - if the data was actually generated with parameter $\\theta_0$ then as $n \\rightarrow \\infty$, $\\hat \\theta_n \\rightarrow \\theta_0$.\n","\n","\n","\n","Extra reading to understand these points better: https://www.math.arizona.edu/~jwatkins/O_mle.pdf"]},{"cell_type":"markdown","metadata":{"id":"vHUfi-jsgGsA"},"source":["#### MLEs and ML\n","\n","Suppose we have data sequence $\\{(x_i, y_i)\\}_{i = 1}^n$ and specify a model of the form\n","$$\n","Y = f(X) + \\epsilon\n","$$\n","\n","where $f \\in \\mathcal{F}$ and $\\epsilon$ is random.\n","\n","What were actually doing is assuming a distribution on $Y \\mid X$\n","\n","For example\n","\\begin{align}\n","Y &= f(X) + \\epsilon \\\\\n","\\epsilon &\\sim N(0, 1)\n","\\end{align}\n","\n","Then\n","$$ Y \\mid X = x \\sim N(f(x), 1)$$\n"]},{"cell_type":"markdown","metadata":{"id":"HTcFrvyep5BU"},"source":["#### Maximizing the likelihood\n","\n","The goal in ML is to find the function $f \\in \\mathcal{F}$ that fits the data as well as possible. We can use maximum likelihood estimation for this.\n","\n","Suppose we want to find $\\hat f = \\arg\\max_{f \\in \\mathcal{F}}\\mathcal{L}(f \\mid \\{(x_i, y_i)\\}_{i = 1}^n)$ again assuming $Y = f(X) + \\epsilon$ with $\\epsilon ~ N(0,1)$ where\n","\n","\\begin{align}\n","\\mathcal{L}(f \\mid \\{(x_i, y_i)\\}_{i = 1}^n) &= \\prod_{i =1}^n \\mathcal{L}(f \\mid (x_i, y_i)) \\\\\n","&= \\prod_{i =1}^n \\frac{1}{\\sqrt{2\\pi}} \\exp \\left(-\\frac{(y_i - f(x_i))^2}{2} \\right).\n","\\end{align}\n","\n","#### Minimization\n","Almost everything we do will involve minimization rather than maximization (by convention).\n","\n","You can (and will) show that maximizing the likelihood function is equivalent to minimizing the __negative log likelihood__ function\n","\n","$$\n","- \\log \\prod_{i =1}^n \\frac{1}{\\sqrt{2\\pi}} \\exp \\left(-\\frac{(y_i - f(x_i))^2}{2} \\right) \\propto \\frac{1}{n} \\sum_{i =1}^n (y_i - f(x_i))^2\n","$$\n","\n","otherwise known as the MSE in this case.\n","\n","\n","We will see that many other loss functions are derived from maximum likelihood, such as classification losses. You've likely performed MLE many times without realizing it."]},{"cell_type":"markdown","metadata":{"id":"9QVnYZ8PFaEy"},"source":["## Summary - How do we formulate the learning problem\n","\n","\n","In a typical problem setup...\n","- We have two random variables $X$ and $Y$.\n","- Only $X$ is available for observation.\n","- Want to predict $Y$ given $X$\n","\n","Assume some model of the form\n","$$Y = f(X) + \\epsilon$$\n","\n","Def. A __predictor__ is any \"well behaved\" function from $X$ to $Y$. Typically call this $f$. We use a predictor to make predictions: $f(X)$\n","\n","Goal: __learn__ a predictor $f$ such that $f(X) \\approx Y$.\n","\n","The main ingredients to specify a typical learning problem are as follows\n","\n","1. __Targets__ $y_1,...,y_n$.\n","  - Otherwise known as labels or dependent variables.\n","  - This is our ultimate object of interest. we want to be able to predict future values say $y_{n+1}$ given $x_{n+1}$.\n","2. __Features__ $x_1,...,x_n$.\n","  - Otherwise known as covariates or independent variables.\n","  - This is the information that we use to predict $y$. In statistical learning, the more covariate information the better. Not as interested in selecting (subsetting) variables as in classical statistics but rather quantifying feature importance.\n","  - We assume there is some relationship between $x_i$ and $y_i$\n","3. The __type of model__ we want to use. Formally, a pre-supposed __family (or class) of predictors__ $\\mathcal{F}$\n","  - These are all the possible predictors we will consider. Different problems require different families of predictors.\n","  - Ex. linear predictors $f \\in \\mathcal{F}$ s.t. $f(x) = \\alpha + \\beta x$\n","  - Impossible to learn an arbitrary predictor from features to targets\n","  - Instead we impose structure on the relationship between features and targets, such as a linear relationship.\n","  - $\\mathcal{F}$ is also known as a hypothesis class\n","4. A __loss__ function $\\mathcal{L}(f, (x, y))$\n","  - Loss functions measure the quality of your predictor. Different loss functions are required for different problems\n","  - The loss function is how we find the \"best\" predictor in our pre-supposed family $\\mathcal{F}$. I.e. the best model of its kind.\n","  - Ex. Squared Error $\\mathcal{L}(f, (x, y)) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - f(x_i))^2$\n","\n","\n","And essentially everything follows through no matter what were doing.\n","1. Identify the quantitiy (variable) to be predicted\n","2. Identify information (variables) that might help predict that quantity\n","3. Choose a model type (linear regression, polynomials, trees, etc.)\n","4. Find the best model of that type according to your loss function\n","\n","Or stated another way we will take a \"universal\" approach to predictive modeling\n","1. Sample training data $\\{(x_i,y_i)\\}_{i = 1}^n$\n","2. Choose a model class $\\mathcal{F} = \\{f \\mid f:X \\mapsto Y \\}$\n","3. Find the best model $\\hat f = \\arg\\min_{f \\in \\mathcal{F}} \\mathcal{L}(f, (x, y))$"]},{"cell_type":"markdown","metadata":{"id":"NDz7ygJ-YJLN"},"source":["### Example 1 -- Linear model\n","\n","\n","Were interested in predicting some target $Y$ using feature $X$. Both are univariate (1 dimension) and plotted against each other."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":558},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1724857180815,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"LUo_-jpcYHe_","outputId":"40215831-ea3c-4dd3-88b3-3b26371c53c1"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"9oRIqpzZhYOq"},"source":["- There is a clear linear relationship between $X$ and $Y$ values\n","- Thus we decide to impose a linear model, i.e. we assume\n","$$ Y = \\alpha + \\beta X + \\epsilon $$\n","We then further assume that $\\alpha = 0$ for simplicity\n","- Implictly we are defining a family of functions\n","$$ \\mathcal{F} = \\{\\beta X : \\beta \\in \\mathbb{R} \\}$$\n","- We want to find the \"best fitting\" member of $\\mathcal{F}$ to match our observations\n","\n","- How to choose? Lets look at some candidates"]},{"cell_type":"markdown","metadata":{"id":"Vv0yyqLR6Lbf"},"source":["We sample random functions by sampling random $\\beta$ values and plotting $X\\beta$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1724857180815,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"bDrh9EpH6T_S","outputId":"2c6c37d4-358d-461b-d103-97335ebaf33f"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":690},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1724857181129,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"HgwnQjjIZ2tl","outputId":"495b6e54-3528-471a-a9cd-562d655c5d2f"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"gDdN2UvYitpu"},"source":["Visually we can see that some of the lines \"fit\" the data well (blue) and others (purple) are terrible\n","\n","Lets quantify the fit of a line numerically with a __loss__ function. We will use the __mean squared error__\n","\n","$$ \\mathcal{L}(f, (x, y)) = MSE(f, (x, y)) =  \\frac{1}{n} \\sum_{i = 1}^n (y - f(x))^2 $$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1724857181129,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"2J1T1bmobovW","outputId":"0a5399ec-b5ee-4511-e995-b25e29a21972"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# example models have a variety of MSEs\n"]},{"cell_type":"markdown","metadata":{"id":"Vrcah6uvjl9j"},"source":["We say that the model with the lowest error is \"best\"\n","\n","- How do we find the best possible model? Minimize the loss function.\n","$$ \\hat{f} = \\arg\\min_{f \\in \\mathcal{F}} \\mathcal{L}(f, (x, y))$$\n","\n","- Specifically for the MSE\n","$$ \\hat{f} = \\arg\\min_{f \\in \\mathcal{F}}  \\frac{1}{n} \\sum_{i = 1}^n (y_i - f(x_i))^2$$\n","\n","- More specifically for our problem (we assume $\\alpha = 0$)\n","$$\\hat{\\beta} = \\arg\\min_{\\beta \\in \\mathbb{R}} \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\beta x_i)^2$$\n","\n","- Which has as an exact solution (least squares solution)\n","$$\\hat{\\beta} = \\frac{\\sum_{i = 1}^n x_i y_i}{\\sum_{i = 1}^n x_i^2}$$\n","\n","- So our optimal function is $\\hat{f}(x) = \\hat{\\beta}x$ (plotted below)\n","\n","\n","In general the loss funtion will not have an exact solution. We will use iterative methods to minimize the loss.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":576},"executionInfo":{"elapsed":576,"status":"ok","timestamp":1724857181700,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"FkmNtedea2Fz","outputId":"e668dfa7-56d5-486e-9318-790338763090"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1724857181700,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"cWpA7PWObPYk","outputId":"7920c1bf-2e1a-4cd7-cb35-723dbc752a6e"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# optimal model will have the lowest error (average loss)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1724857181700,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"oWMFERz9btsp","outputId":"54f108dd-cac3-47dd-b15d-3c3754efcc29"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# for reference (random guesses)\n"]},{"cell_type":"markdown","metadata":{"id":"uJ_DAmyxcynG"},"source":["### Example 2 -- Wave model\n","\n","\n","Were interested in predicting some target $Y$ using feature $X$. Both are univariate (1 dimension) and plotted against each other.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":558},"executionInfo":{"elapsed":690,"status":"ok","timestamp":1724857182385,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"dVjhg7fBc4CS","outputId":"c35e28f5-5dbf-4df0-9d8a-5a6dd87edd1e"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"rKijcR5ymEEV"},"source":["This time we notice that the relationship between $X$ and $Y$ is more complicated\n","\n","We suspect there is a __sinusoidal__ relationship so we will restrict ourselves to fitting __sin__ functions\n","\n","\n","- Thus we decide to impose a sinusoidal model, i.e. we assume\n","$$ Y = \\alpha + \\beta \\sin(3 \\pi X) + \\epsilon $$\n","and assume $\\alpha = 0$ for simplicity\n","- Implictly we are defining a family of functions\n","$$ \\mathcal{F} = \\{\\beta \\sin(3 \\pi X) : \\alpha \\in \\mathbb{R}, \\beta \\in \\mathbb{R} \\}$$\n","- We want to find the \"best fitting\" member of $\\mathcal{F}$ to match our observations\n","\n","- How to choose? Lets look at some candidates"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":648},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1724857182385,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"tAFTm53OdBsM","outputId":"068bba6a-67b3-4c32-986a-c467446f2e30"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"eYxP9x8mm7rA"},"source":["Visually we can see that some of the lines \"fit\" the data well (blue) and others (purple) are terrible\n","\n","We again use the __mean squared error__ to quantify the overall \"fit\" of the candidate models\n","\n","$$ \\mathcal{L}(f, (x, y)) = MSE(f, (x, y)) =  \\frac{1}{n} \\sum_{i = 1}^n (y_i - f(x_i))^2 $$\n","\n","Or more specifically\n","\n","$$ MSE(f, (x, y)) =  \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\beta \\sin(3 \\pi x_i))^2 $$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1724857182385,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"7pNXWFwVe3PY","outputId":"c8e3520e-32dc-4299-e891-b1ab7c47d678"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["\n","\n","# example models have a variety of errors (average loss)\n"]},{"cell_type":"markdown","metadata":{"id":"sSerFpnPnhv8"},"source":["- Which again has as an exact solution (least squares solution) assuming $\\alpha = 0$\n","$$\\hat{\\beta} = \\frac{\\sum_{i = 1}^n \\sin(3 \\pi x_i) y_i}{\\sum_{i = 1}^n \\sin(3 \\pi x_i)^2}$$\n","\n","- So our optimal function is $\\hat{f}(x) = \\hat{\\beta}\\sin(3 \\pi x_i)$ (plotted below)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":576},"executionInfo":{"elapsed":438,"status":"ok","timestamp":1724857182817,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"EqIrhvmreVpu","outputId":"29638450-05bb-4265-fcf1-a693373dd2f3"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1724857182818,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"YXb_ol0oe9Rr","outputId":"518cb023-2d40-4082-e731-72e90a38481b"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["\n","# optimal model will have the lowest error (average loss)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1724857182818,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"NBpNYsNzfPER","outputId":"f3a2d35f-9fe2-4026-8afe-b329a57a8e77"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# example models have a variety of errors (average loss)\n"]},{"cell_type":"markdown","metadata":{"id":"pmVHxNUnfQ2Y"},"source":["### Example 1 vs Example 2\n","\n","- In both cases we colleceted features $x_1,...,x_n$ and targets $y_1,...,y_n$\n","\n","- In example 1, we plotted the data and suspected a __linear__ relationship so we restricted our search to __linear__ functions of $X$ and $Y$\n","\n","- In example 2, the relationship was __sinusoidal__ so we restricted our search to __sin__ functions\n","\n","- Both cases used a mean squared error loss to find the optimal predictor from the pre-defined class of predictors"]},{"cell_type":"markdown","metadata":{"id":"HG_HiSCwr4rs"},"source":["# 2. Evaluating models\n","\n","> learning what the machine learns\n","\n","The central focus of machine learning is teaching our model to make accurate predictions.\n","- It does not matter how complex your model is\n","- Or that you understand how exactly it works\n","- We just care that it continues to _predict well in the future_\n","\n","\n","The problem we encounter is that machine learning models have become very complex\n"," - Models like ChatGPT have billions of parameters\n"," - And are trained on millions of observations\n","\n","How do we know if our model is going to keep working well?\n","- In general you don't (anything could change)\n","- But we can get an idea of when it will and wont fail"]},{"cell_type":"markdown","metadata":{"id":"AekSQlyxA0W_"},"source":["There are really three issues that we focus on.\n","\n","1. **Accuracy** - How accurate is my model? Quantify our ability to predict $Y$ given $X$ on average. We will use the term accuracy to generically refer to our loss\n","3. **Generalizability** Will my model continue working in the future? If we observe new observation will our model still be accurate and reliable?\n","4. **Inference** What conclusions can I draw about the relationship between $X$ and $Y$ from my model?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jsOXSKfJwIZN"},"source":["### Part 1 Accuracy\n","\n","Given target values $y_1,...,y_n$ and predictions $\\hat y_1, ..., \\hat y_n$, where $\\hat y_i = \\hat f(x_i)$, **accuracy** genearlly refers to the \"similarity\" between each $y_i$ and $\\hat y_i$.\n","\n","Examples:\n","- Regression - suppose each $y_i, \\hat y_i \\in \\mathbb{R}$ for $i \\in {1,...,n}$. A common accuracy score for regression problems is the mean squared error (MSE).\n","$$MSE = \\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat y_i)^2$$\n","\n","- Classification - Suppose each $y_i \\in {0, 1}$ and each $\\hat y_i \\in [0, 1]$. A common accuracy score for classification problems is the Binary Cross Entropy (BCE).\n","$$BCE = -\\frac{1}{n}\\sum_{i = 1}^n y_i\\log(\\hat y_i) + (1 - y_i)\\log(1 - \\hat y_i)$$\n","\n","\n","Essentially we will measure accuracy with the loss function. Typically lower values of the loss function mean your model is more accurate."]},{"cell_type":"markdown","metadata":{"id":"UIJ2X6lp0Pbw"},"source":["## Part 2 Generalization\n","\n","Generalization refers to the ability of our model to continue working _in the future_\n","\n","Suppose\n","- We learn model $\\hat f$ with loss function $\\mathcal{L}$ on data sequence $Z_{train} = \\{ (x_i, y_i) \\}_{i=1}^n$\n","\n","- The training loss is $\\mathcal{L}(\\hat f, Z_{train})$ is small enough that we think the model fits the data well\n","\n","What if we observe another data sequnce?\n","$Z_{test} = \\{ (x_i, y_i) \\}_{i={n+1}}^{n + m}$\n","\n","Will $\\mathcal{L}(\\hat f, Z_{test})  \\approx \\mathcal{L}(\\hat f, Z_{train})$?"]},{"cell_type":"markdown","metadata":{"id":"lfpp2B36yf9d"},"source":["#### Notation\n","\n","1. We call $Z_{train} = \\{ (x_i, y_i) \\}_{i=1}^n$ the _training data_ because its the data we use to estimate $f$\n","\n","2. We call $Z_{test} = \\{ (x_i, y_i) \\}_{i={n+1}}^{n + m}$ the _testing data_ because its the data we use to test if $f$ will generalize"]},{"cell_type":"markdown","metadata":{"id":"9l8vsDImyV1L"},"source":["Our first thought may be to look at the training loss\n","- We minimized the training loss on a large dataset so that our model would fit the data well\n","- If it fit this data well shouldn't it fit future data well?\n","\n","Lets look at an example"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":670},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1724857182818,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"-6HvJQNqyU1j","outputId":"89ee9ee1-c34f-4f6e-b6ff-393fdded8bb3"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4O7Bmq5IzTkO"},"source":["We observe the grey datapoints and want to model the relationship between $X$ and $Y$\n","- We dont see the true relationship (blue line)\n","- So lets try a few models out\n","\n","We'll try\n","1. Linear model\n","2. A 6th degree polynomial\n","3. A 12th degree polynomial"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":706},"executionInfo":{"elapsed":326,"status":"ok","timestamp":1724857183137,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"VXJvfqe2xibm","outputId":"30fd1522-9958-4c31-b35b-81bca0344267"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1724857183137,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"t_ZSdbsHbpWh","outputId":"fe15e71a-57dd-4b78-8e28-6bb13018cd7d"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"p4bpvh140pHL"},"source":["On our initial dataset the linear model was __worst__ and the 12th degree polynomial was __best__\n","- On the data we trained on the most complicated model performed the best\n","- What about on future data?\n","\n","Lets suppose we observe some more data\n","- `x_test` and `y_test`\n","- How do the three models perform?\n","\n","\n","We apply each model (trained on the initial dataset) to the new data `x_test` and `y_test` without refitting the model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":706},"executionInfo":{"elapsed":476,"status":"ok","timestamp":1724857183608,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"D4-R_gA1zyKZ","outputId":"e3eb67cf-62e1-4be1-9672-931db1a08f5a"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1724857183608,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"MQoL7xR4ya8i","outputId":"ecbdd6a9-48c3-46aa-ea04-fefff9a1f42d"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# test error\n"]},{"cell_type":"markdown","metadata":{"id":"aJlUy3Ew1sRB"},"source":["- Now the order is reversed!?\n","\n","- Our best model, the 12th degree polynomial is now the worst and linear is the best\n","\n","- Is this a coincidence?\n","\n","- Lets check again. We again apply the model (trained on the initial dataset) to new data `x_test` and `y_test` without refitting the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1724857183608,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"fruSWov71f8u","outputId":"a618d23b-f43b-40dd-88a2-97261bd66f02"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["### test on test\n"]},{"cell_type":"markdown","metadata":{"id":"z331j09D2XME"},"source":["- Even on this third dataset the linear model is holding strong.\n","\n","- However\n"," - Two tests dont prove anything\n"," - Lets try 200 tests and average the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PU5QTb52SnI"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":562,"status":"ok","timestamp":1724857184165,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"Lut1SJEI2QNj","outputId":"5419f25a-1676-4046-b07c-4495e984f246"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1724857184165,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"TPPfbLXU21oZ","outputId":"083d30c0-b038-4ac5-971d-e26a367b3585"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZE8bfjuhbqAY"},"source":["Clearly its not enough that our model performs well on the training dataset. Our best performer on training ended up being the worst performer on all of the \"future\" datasets"]},{"cell_type":"markdown","metadata":{"id":"RE5H8hDa9HJQ"},"source":["### Fitting and Overfitting\n","\n","Why is this the case? There is phenomenon called __overfitting__ that can (and likely will) happen.\n","\n","Overfitting means that our learned model is too specialized to the specific dataset that we trained on\n","- Essentially our model started fitting the \"noise\" in the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":819},"executionInfo":{"elapsed":1866,"status":"ok","timestamp":1724857186024,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"LNtjhb4W-ihB","outputId":"9580b11e-b424-40a4-f469-60f6e398e3cd"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Iuo4uDPj_d8m"},"source":["In this case the true model is a linear model with gaussian white noise around it.\n","- Our linear model came closest to capturing the underlying \"structure\" of the model. I.e. the true mean of the conditional distribution.\n","- the 6th and 12th degree polynomial deviate from the True model because they are trying to fit the individual data points, i.e. noise about the true model\n","\n","\n","How can we know a situation like this is happening without knowing the true model?\n","\n","Sample splitting and checking the __generalization gap__\n","- Split the data into two distinct datasets (train and test)\n","- Check the difference between training and testing error\n"]},{"cell_type":"markdown","metadata":{"id":"vLXJfYwq9IXZ"},"source":["## Improving Generalization\n","\n","Our ultimate goal is to have models that generalize well. There are many strategies that have been developed that can help improve generaliztion.\n","\n","Some that we will discuss and use\n","\n","1. Sample splitting\n","2. Bias / Variance considerations\n","3. Regularization\n","\n","\n","Often we will use all three simultaneously"]},{"cell_type":"markdown","metadata":{"id":"zVKCTYCcrVH3"},"source":["### 1. Sample splitting\n","\n","The best (and easiest) way to evaluate how well our model performs in the future is to simply evaluate it on some \"future\" data\n","- Because its the present we don't have future data\n","- But we can emulate it through __sample splitting__"]},{"cell_type":"markdown","metadata":{"id":"v2xYzoIV4pja"},"source":["\n","A common approach is to randomly split the data into two datasets\n"," - Train: We use this data to fit (train) our model\n"," - Test: We use this data to evaluate (test) our model\n","\n","\n","Essentially all training will be performed on the training dataset and we will reserve the test set for evaluation\n"," - training is sometimes called \"in sample\" and test is called \"out of sample\"\n"]},{"cell_type":"markdown","metadata":{"id":"B2gJkWFM-qTy"},"source":["The intuition is that because the model will have never seen the \"test\" data it is, in no way, fitted to the test data.\n","- $\\hat f$ and $Z_{test} = \\{ (x_i, y_i) \\}_{i={n+1}}^{n + m}$ are independent of each other\n","- note: a learned model $\\hat f$ is itself a random variable since $\\hat f$ is a function of the training data $Z_{train}$ which is random.\n","- Then $\\mathcal{L}(\\hat f, Z_{test})$ is a good approximation for performance on unseen future data because it effectively is unseen future data"]},{"cell_type":"markdown","metadata":{"id":"Kn1imZjmzv0M"},"source":["#### Train, Test, Validation\n","\n","Another approach is to randomly split the data into three datasets\n"," - Train, test, and validation\n"," - Validation acts like a test set when were building our model\n","\n","You train the model on the training set and check \"out of sample\" performance on the validation set. You keep repeating this until you're happy with your loss on validation. At the end you use the test set once to verify that the model generalizes.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V4c7Xv631OOG"},"source":["#### Why is a validation set necessary?\n","\n","- Lets say you train your model $\\hat f$ on $Z_{train} = \\{ (x_i, y_i)\\}_{i=1}^n$\n","- $\\mathcal{L}(\\hat f, Z_{train})$ looks good but $\\mathcal{L}(\\hat f, Z_{val})$ is too high\n","\n","- So you go back and modify $f$ and refit on $Z_{train}$\n","\n","- $\\mathcal{L}(\\hat f, Z_{train})$ looks good but $\\mathcal{L}(\\hat f, Z_{val})$ is still too high\n","\n","- So you go back and modify $f$ ...\n","\n","\n","Eventually you're happy with $\\mathcal{L}(\\hat f, Z_{train})$ and $\\mathcal{L}(\\hat f, Z_{val})$, but there is an issue!\n","\n","> $\\hat f$ is no longer independent of the validation set. You used the validation set (indirectly) to train $\\hat f$\n","\n","$\\mathcal{L}(\\hat f, Z_{val})$ is no longer an unbiased estimate of the out of sample error. You may very well have overfit to $Z_{train}$ and $Z_{val}$ simultaneously. However, because you've never used $Z_{test}$ you can still use $\\mathcal{L}(\\hat f, Z_{val})$ to estimate the true out of sample error."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1724857186025,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"JA5laNNu9R54","outputId":"de5f8aed-6579-45e6-eed7-83e5389387fd"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# classic example of high degree poly failing to generalize?\n","# from numpy.polynomial.polynomial import Polynomial\n"]},{"cell_type":"markdown","metadata":{"id":"d4jfgjmh9zGa"},"source":["- All features of the datset need to be included in the training and testing dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1724857186025,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"NlFIMDr69_mF","outputId":"58493faa-574a-445a-8b71-3d2c5748e0f9"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"0DlfvWV2-ps7"},"source":["- In the original dataset there were `n = 200` observations and `p = 10` features\n","- Using a 70/30 split\n","  - train has `n = 140` obs and `p = 10` features\n","  - train has `n = 60` obs and `p = 10` features\n","\n","- Always split the rows not the columns (except in special circumstances)\n","\n","\n","To get train, test, validation just split your first training dataset into a train and test set.\n","\n","```\n","           Data\n","         /      \\\n","    \"train\"     test\n","   /       \\\n","Train      Val\n","```\n","\n","Most important consider is the number of observations in each set (more is better)\n","- Typical split may be 50\\% Train, 25\\% Val, 25\\% Test\n","- Proportions not that important\n","- raw number of samples more important\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sZLuAOOT4r7h"},"source":["#### How does sample splitting help?\n","\n","The test data is never seen by the model during the fitting process.\n"," - Performance on test is far more indicative of future performance than performance on train.\n"," - Because test essentially is \"future\" data\n"," - Intuition: If the model generalizes from train to test then its likely that it will generalize again to any future dataset\n","\n","Drawback: training on less data usually results in a worse model. Need to ensure there is enough data to train an adequate model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1724857186025,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"Ei5IovJqAwtc","outputId":"f1a9c0ad-1ac7-4b5a-ee6b-9ecdd2ca26c2"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# classic example of high degree poly failing to generalize?\n","# from numpy.polynomial.polynomial import Polynomial\n","np.random.seed(3)\n","\n","n = 200\n","beta = 0.5\n","\n","x = np.linspace(-3, 3, n)\n","y = beta * x + np.random.normal(0, 3, n)\n","\n","# 70/30 train/test split\n","idx = np.random.choice(range(n), int(n*0.7), replace=False)\n","mask = np.ones(n, dtype=bool)\n","mask[idx] = False\n","\n","x_train = x[mask]\n","y_train = y[mask]\n","x_test = x[~mask]\n","y_test = y[~mask]\n","\n","\n","### fit on train\n","model1 = np.polyfit(x_train, y_train, deg = 1)\n","model2 = np.polyfit(x_train, y_train, deg = 6)\n","model3 = np.polyfit(x_train, y_train, deg = 12)\n","\n","yhat1_train = np.polyval(model1, np.sort(x_train))\n","yhat2_train = np.polyval(model2, np.sort(x_train))\n","yhat3_train = np.polyval(model3, np.sort(x_train))\n","\n","mse1_train = np.mean((y_train - yhat1_train)**2)\n","mse2_train = np.mean((y_train - yhat2_train)**2)\n","mse3_train = np.mean((y_train - yhat3_train)**2)\n","\n","### evaluate on test\n","yhat1_test = np.polyval(model1, np.sort(x_test))\n","yhat2_test = np.polyval(model2, np.sort(x_test))\n","yhat3_test = np.polyval(model3, np.sort(x_test))\n","\n","mse1_test = np.mean((y_test - yhat1_test)**2)\n","mse2_test = np.mean((y_test - yhat2_test)**2)\n","mse3_test = np.mean((y_test - yhat3_test)**2)\n","\n","error_df = pd.DataFrame(index = ['Train Error', 'Test Error', 'Gen. Gap'])\n","error_df['Linear'] = [mse1_train, mse1_test, np.abs(mse1_train - mse1_test)]\n","error_df['6th Poly'] = [mse2_train, mse2_test, np.abs(mse2_train - mse2_test)]\n","error_df['12th Poly'] = [mse3_train, mse3_test, np.abs(mse3_train - mse3_test)]\n","\n","# # generalization gap. Smaller is better. Big differences (usually) mean you way overfit the data\n","print(error_df)"]},{"cell_type":"markdown","metadata":{"id":"dI1QrIvEBjX1"},"source":["- A \"big\" gap between Train Error and Test Error indicates that you (likely) overfit your data\n","- The word \"big\" depends on context, so there is no rule\n","- Between two models with equal training error we prefer the one that has a smaller gap"]},{"cell_type":"markdown","metadata":{"id":"fHa1yR6RrVH5"},"source":["### 2. Bias and variance trade-off\n","\n","Assume our general model form $$Y = f(X) + \\epsilon$$ where $E[\\epsilon] = 0$ and $Var(\\epsilon) = \\sigma^2$\n","\n","We learn model $\\hat f$ with loss function $\\mathcal{L}$ on data sequence $Z_{train} = \\{ (x_i, y_i) \\}_{i=1}^n$\n","\n","Given a new data point $x_0$ we decompose the average prediction error"]},{"cell_type":"markdown","metadata":{"id":"uaKSCFg2FsQA"},"source":["$$\n","\\begin{align}\n","\\text{MSE}(f, (x, y)) &= E[y - \\hat y]^2 \\\\\n"," &= E[f(x_0) + \\epsilon - \\hat f(x_0)]^2 \\\\\n"," &= E[f(x_0) - \\hat f(x_0)]^2 + E[\\epsilon]^2 + \\underbrace{2E[f(x_0) + \\hat f(x_0)]E[\\epsilon]}_{E[\\epsilon] = 0} \\\\\n"," &= E[f(x_0) - \\hat f(X)]^2 + E[\\epsilon]^2\n","\\end{align}\n","$$\n","\n","This last part decomposes as\n","\n","$$\n","\\underbrace{E[f(x_0) - \\hat f(x_0)]^2}_{\\text{Reducible error}} + \\underbrace{Var[\\epsilon]}_{\\text{Irreducible error}}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Ky4gp7WEFA9s"},"source":["Already we see that our loss function decomposes into two parts\n","1. Reducible Error - This error between your fitted function and the true underlying. With better models we can hopefully reduce this to 0.\n","2. Irreducible Error - This is the error intrinsic to our model which assumed there was error in the data\n","\n","As any self help book will tell you: we shouldn't focus on the things we cant change (irreducible error)\n","\n","So lets focus on the what we can control: The reducible error\n"]},{"cell_type":"markdown","metadata":{"id":"bryxTx_fITyW"},"source":["$$\n","\\begin{aligned}\n"," E[f(x_0) - \\hat f(x_0)]^2 &=  E[f - E[\\hat f] + E[\\hat f] - \\hat f)]^2 \\\\\n"," &=  E[f - E[\\hat f]]^2 + E[E[\\hat f] - \\hat f]^2 + 2E[f - \\hat f]E[E[\\hat f] - \\hat f]] \\\\\n","  &=  E[f - E[\\hat f]]^2 + E[E[\\hat f] - \\hat f]^2 \\\\\n","  &=  \\underbrace{(f - E[\\hat f])^2}_{\\text{Bias}(\\hat f)^2} + Var[\\hat f]\n","\\end{aligned}\n","$$\n","\n","So we have decomposed the reducible error into two components. Bring back the $f(x_0)$ notation we have\n","$$\n","\\text{Reducible Error} = \\text{Bias}(\\hat f(x_0))^2 + Var[\\hat f(x_0)]\n","$$\n","\n","- Bias measures the difference between the expected prediction and the underlying true model\n","\n","- Variance measures how certain we are that were close to the true model\n"," - i.e. if we trained on a new dataset is our model still likely to be close to the truth?"]},{"cell_type":"markdown","metadata":{"id":"liylWCgKrVH7"},"source":["Explanation:\n","\n","Recall that the fitted model $\\hat f$ is a random variable. Given a random training dataset $Z_{train}$ we learn a random model $\\hat f$ via the loss function $\\mathcal{L}$\n","\n","\n","__Bias__ tells us\n"," - Given a random training dataset $Z_{train}$, how close __on average__ will the learned function $\\hat f$ be to the true underlying function $f$\n"," - i.e. imagine you had 100 datasets $$Z^1_{train},...,Z^{100}_{train}$$\n"," - you train 100 models $$\\hat{f}^1,...,\\hat{f}^{100}$$\n"," - bias $\\approx \\frac{1}{100} \\sum_{i=1}^n [f(x_i) - \\hat{f}^i(x_0)]$\n","\n","__Variance__ tells us\n","- Given a random training dataset $Z_{train}$, how much will the learned functions $\\hat f$ deviate from each other\n"," - i.e. imagine you had 100 datasets $$Z^1_{train},...,Z^{100}_{train}$$\n"," - you train 100 models $$\\hat{f}^1,...,\\hat{f}^{100}$$\n"," - variance $\\approx \\frac{1}{n} \\sum_{i=1}^n [\\hat{f}^i(x_0) - \\bar{f}(x_0)]^2$ where $\\bar{f}(x_0) = \\sum_{i=1}^n \\hat{f}^i(x_0)$\n","\n","Bias measures the how well optimized models from our class $\\mathcal{F}$ perform on average across different training sets\n","\n","Variance measures how similar optimized models are from our class $\\mathcal{F}$ on average across different training sets\n","\n","We want models that are\n","1. Accurate (low bias)\n","2. Highly stable (low variance)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1724857186025,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"QrrJ1vQAfhMq","outputId":"13e2fe3a-7f2f-48ef-8946-34f5bf88174b"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["from IPython.display import Image, display\n","Image('bias_variance_2.png')"]},{"cell_type":"markdown","metadata":{"id":"DtltNseXDXXJ"},"source":["Which of these do you think is best?\n","\n","<div>\n","<img src=\"https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png\" width=\"500\" height=\"450\"/>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"ntqbnhq0HB0m"},"source":["#### Model Complexity and Bias-Variance trade off\n","\n","The idea of a tradeoff between \"bias\" and \"variance\" of our prediction algorithm comes from inspection of the mean squared error loss $MSE$.\n","\n","- A different loss function will not necesasrily give such a nice decomposition\n","- What if I use a different loss? Will the principle hold?\n","\n","\n","Yes\n","\n","There will always be some tradeoff between your training and testing error\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":517},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1724857186025,"user":{"displayName":"Scott Bruce","userId":"12687994165436531917"},"user_tz":300},"id":"ONoYTq4pfu-y","outputId":"45b4ca5b-9487-4861-a489-845ba6074913"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["Image('bias_variance_3.png', width = 700, height = 500)"]},{"cell_type":"markdown","metadata":{"id":"sqWAC9FSll47"},"source":["We can use the idea of __model complexity__ to represent this trade off for any loss function and model family\n","\n","Statistical Learning Theory tells us\n"," - More complex models will fit the training data bettter (low bias)\n"," - Less complex models will generalize better\n"]},{"cell_type":"markdown","metadata":{"id":"Yrq6iyEpnYRA"},"source":["What is model complexity?\n"," - We abstractly think of this as the models __capacity__ to fit the data\n"," - Bigger model families $\\mathcal{F}$ allow for more complexity\n","\n","\n","Theory:\n","\n","Given a loss function $\\mathcal{L}$ and training data $Z_{train} = \\{(x_i,y_i)\\}_{i=1}^n$, suppose we have two model families $\\mathcal{F}_1$ and $\\mathcal{F}_2$ such that\n","$$\\mathcal{F}_1 \\subset \\mathcal{F}_2$$\n","\n","Example: \\\\\n","$\\mathcal{F}_1$ is all linear models $\\alpha + \\beta_1 X$ \\\\\n","$\\mathcal{F}_2$ is all quadratic models $\\alpha + \\beta_1 X + \\beta_2 X^2$\n","\n","Any model in $\\mathcal{F}_1$ is representable as a model in $\\mathcal{F}_2$ by setting $\\beta_2 = 0$.\n","\n","Then\n","$$\\min_{f \\in F_2} \\mathcal{L}(f, Z_{train}) \\leq \\min_{f \\in F_1} \\mathcal{L}(f, Z_{train}) $$\n","\n","\n","This means that if we have a big family of models that contains a smaller family of models then the big family will always have an equal or better minimizer of the training loss than the small family\n","- By chosing a bigger family you will never increase the training error\n","- Up to numerical issues of course\n","\n","\n","THIS DOES NOT MEAN\n","$$\\min_{f \\in F_2} \\mathcal{L}(f, Z_{test}) \\leq \\min_{f \\in F_1} \\mathcal{L}(f, Z_{test}) $$\n","\n","<!--\n","__Example__: \\\\\n","$\\mathcal{F}_1$ is all linear models $\\alpha + \\beta_1 X$ \\\\\n","$\\mathcal{F}_2$ is all cubic models $\\alpha + \\beta_1 X + \\beta_2 X^2 + \\beta_3 x^3$ -->"]},{"cell_type":"markdown","metadata":{"id":"XcR0az7JPln6"},"source":["## Conclusion\n","\n","1. Evaluating on your training dataset is not enough. Otherwise you suffer from overfitting\n","2. You need to measure out of sample (test) accuracy\n","   - We do this with sample splitting (train/test) split\n","3. The loss gives us insight into model behavior\n","   - Bias and Variance describe accuracy on train and model complexity\n","   - We want model that are accurate (low bias) and generalize well (low variance)\n","4. The model complexity gives us insight into performance\n"," - Balance complexity vs robustness for optimal out of sample predictive skill"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
